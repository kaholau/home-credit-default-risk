{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet \n",
    "\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df_old):\n",
    "    df = df_old.copy()\n",
    "    \n",
    "    column_list = [\"NAME_TYPE_SUITE\", \"AMT_ANNUITY\", \"AMT_GOODS_PRICE\", \"OCCUPATION_TYPE\"]\n",
    "    # For categorical column, fill missing value with mode\n",
    "    # For numerical column, fill missing value with median\n",
    "    for col in column_list:\n",
    "        if (df[col].dtypes == \"object\"):\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        elif (df[col].dtypes == \"float64\"):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        else:\n",
    "            print(df[col].dtypes)\n",
    "            \n",
    "    # For DAYS_EMPLOYED column\n",
    "    # Description: How many days before the application the person started current employment\n",
    "    # This column contains 55374 entries of value 365243 which does not make sense\n",
    "    # Add a new column called DAYS_EMPLOYED_ANOMALOUS to capture these 55374 entries\n",
    "    # Replace all 365243 with Nan and fill nan with median\n",
    "    df['DAYS_EMPLOYED_ANOMALOUS'] = (df[\"DAYS_EMPLOYED\"] == 365243)\n",
    "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].fillna(df['DAYS_EMPLOYED'].mean())\n",
    "    \n",
    "    # For OWN_CAR_AGE column\n",
    "    # Description: Age of client's car\n",
    "    # Decided to fill all missing value with the max which is 91\n",
    "    # That means we assume that they either don't have a car or own a very old car\n",
    "    df['OWN_CAR_AGE'] = df['OWN_CAR_AGE'].fillna(df['OWN_CAR_AGE'].max())\n",
    "    \n",
    "    # For CNT_FAM_MEMBERS column\n",
    "    # Description: How many family members does client have\n",
    "    # Only 2 missing value so use the mode of family member count which is 2 family members to fill it\n",
    "    df['CNT_FAM_MEMBERS'] = df['CNT_FAM_MEMBERS'].fillna(2)\n",
    "    \n",
    "    # For DAYS_LAST_PHONE_CHANGE column\n",
    "    # Description: How many days before application did client change phone\n",
    "    # Only 1 missing value so use the mode which is 0 to fill it\n",
    "    df['DAYS_LAST_PHONE_CHANGE'] = df['DAYS_LAST_PHONE_CHANGE'].fillna(0)\n",
    "    \n",
    "    # Replace all missing/NA values with 0 for the number of enquiries before application\n",
    "    df['AMT_REQ_CREDIT_BUREAU_YEAR'] = df['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(0)\n",
    "    df['AMT_REQ_CREDIT_BUREAU_HOUR'] = df['AMT_REQ_CREDIT_BUREAU_HOUR'].fillna(0)\n",
    "    df['AMT_REQ_CREDIT_BUREAU_DAY'] = df['AMT_REQ_CREDIT_BUREAU_DAY'].fillna(0)\n",
    "    df['AMT_REQ_CREDIT_BUREAU_WEEK'] = df['AMT_REQ_CREDIT_BUREAU_WEEK'].fillna(0)\n",
    "    df['AMT_REQ_CREDIT_BUREAU_MON'] = df['AMT_REQ_CREDIT_BUREAU_MON'].fillna(0)\n",
    "    df['AMT_REQ_CREDIT_BUREAU_QRT'] = df['AMT_REQ_CREDIT_BUREAU_QRT'].fillna(0)\n",
    "    df['AMT_REQ_CREDIT_BUREAU_YEAR'] = df['AMT_REQ_CREDIT_BUREAU_YEAR'].fillna(0)\n",
    "    \n",
    "    # For EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 columns\n",
    "    # Since the value is already normalized, fill all missing value with mean\n",
    "    df['EXT_SOURCE_1'] = df['EXT_SOURCE_1'].fillna(df['EXT_SOURCE_1'].mean())\n",
    "    df['EXT_SOURCE_2'] = df['EXT_SOURCE_2'].fillna(df['EXT_SOURCE_2'].mean())\n",
    "    df['EXT_SOURCE_3'] = df['EXT_SOURCE_3'].fillna(df['EXT_SOURCE_3'].mean())\n",
    "    \n",
    "    # For OBS_30_CNT_SOCIAL_CIRCLE, DEF_30_CNT_SOCIAL_CIRCLE,\n",
    "    # OBS_60_CNT_SOCIAL_CIRCLE and DEF_60_CNT_SOCIAL_CIRCLE columns\n",
    "    # Description: How many observation of client's social surroundings with observable/defaulted\n",
    "    # Use the mode which is 0 to fill these 4 columns\n",
    "    df['OBS_30_CNT_SOCIAL_CIRCLE'] = df['OBS_30_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    df['DEF_30_CNT_SOCIAL_CIRCLE'] = df['DEF_30_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    df['OBS_60_CNT_SOCIAL_CIRCLE'] = df['OBS_60_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    df['DEF_60_CNT_SOCIAL_CIRCLE'] = df['DEF_60_CNT_SOCIAL_CIRCLE'].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_all_missing_values(grand_table):\n",
    "    value_info = {}\n",
    "    values_to_fill = pd.read_csv('missing_value_for_agg.csv')\n",
    "    for index, row in values_to_fill.iterrows():\n",
    "        table = row['Table']\n",
    "        row_name = row['Row']\n",
    "        row_type = row['Type']\n",
    "        if not value_info.get(table):\n",
    "            value_info[table] = {}\n",
    "        value_info[table][row_name] = (row_type, row['missing value after aggregation'])\n",
    "    \n",
    "    for table in value_info.keys():\n",
    "        # reverse sort so that \"AB_CD\" appears before \"AB\", requried for later processing\n",
    "        columns = list(sorted(value_info[table].keys(), reverse=True))\n",
    "        value_info[table]['all_columns'] = columns\n",
    "        \n",
    "    def fill_value(df, column, value):\n",
    "        if value == 'median':\n",
    "            df[column] = df[column].fillna(df[column].median())\n",
    "        else:\n",
    "            df[column] = df[column].fillna(int(value))\n",
    "        return df\n",
    "    \n",
    "    def process_column(df, column, table):\n",
    "        for col_name in value_info[table]['all_columns']:\n",
    "            if col_name in column:\n",
    "                return fill_value(df, column, value_info[table][col_name][1])\n",
    "        print(\"Column %s in table %s not processed\" % (column, table))\n",
    "        \n",
    "    def process(df):\n",
    "        for column in df.columns:\n",
    "            column = str(column)\n",
    "\n",
    "            if not df[column].isnull().values.any():  # no missing value\n",
    "                continue\n",
    "            if column[0:len('prev_app')] == 'prev_app':\n",
    "                df = process_column(df, column, 'previous_application.csv')\n",
    "            elif column[0:len('install')] == 'install':\n",
    "                df = process_column(df, column, 'installments_payments.csv')\n",
    "            elif column[0:len('credit_bal')] == 'credit_bal':\n",
    "                df = process_column(df, column, 'credit_card_balance.csv')\n",
    "            elif column[0:len('pos_cash')] == 'pos_cash':\n",
    "                if 'CNT_INSTALMENT_FUTURE' in column:  # this column is an ID, should drop\n",
    "                    df = df.drop(column, axis=1)\n",
    "                    continue\n",
    "                df = process_column(df, column, 'POS_CASH_balance.csv')\n",
    "            elif column[0:len('bureau')] == 'bureau':\n",
    "                df = process_column(df, column, 'bureau.csv')\n",
    "            elif column[0:len('client_bureau_balance')] == 'client_bureau_balance':\n",
    "                # remove all columns from MONTHS_BALANCE except for 'MONTHS_BALANCE_min' because\n",
    "                # MONTHS_BALANCE is like an ID, but its min has meaning\n",
    "                if 'MONTHS_BALANCE' in column and 'MONTHS_BALANCE_min' not in column:\n",
    "                    df = df.drop(column, axis=1)\n",
    "                    continue\n",
    "                df = process_column(df, column, 'bureau_balance.csv')\n",
    "            else:\n",
    "                print(\"column %s from grand table not processed\" % column)\n",
    "        return df\n",
    "    \n",
    "    return process(grand_table)\n",
    "\n",
    "def label_encoding(df_old):\n",
    "    df = df_old.copy()\n",
    "    \n",
    "    # For CODE_GENDER column, group 'XNA' categorical value to 'Female' which is the mode since there is only 4 rows\n",
    "    df['CODE_GENDER'] = df['CODE_GENDER'].replace({'XNA': 'F'})\n",
    "    \n",
    "    category_map = {\n",
    "        'CODE_GENDER': {'F': 0, 'M': 1},\n",
    "        'FLAG_OWN_CAR': {'N': 0, 'Y': 1},\n",
    "        'FLAG_OWN_REALTY': {'N': 0, 'Y': 1},\n",
    "        'NAME_EDUCATION_TYPE': {\n",
    "            'Academic degree': 0, \n",
    "            'Lower secondary': 1,\n",
    "            'Secondary / secondary special': 2,\n",
    "            'Incomplete higher': 3,\n",
    "            'Higher education': 4\n",
    "        },\n",
    "        'DAYS_EMPLOYED_ANOMALOUS': {False: 0, True: 1},\n",
    "    }\n",
    "    for col in category_map.keys():\n",
    "        df[col] = df[col].map(category_map[col])\n",
    "    \n",
    "    \"\"\"\n",
    "    encoding_order = {\n",
    "        'CODE_GENDER'     : ['F', 'M'],\n",
    "        'FLAG_OWN_CAR'    : ['N', 'Y'],\n",
    "        'FLAG_OWN_REALTY' : ['N', 'Y'],\n",
    "        'NAME_EDUCATION_TYPE' : ['Academic degree', 'Lower secondary', 'Secondary / secondary special', \n",
    "                                 'Incomplete higher', 'Higher education'],\n",
    "    }\n",
    "    le_features = list(encoding_order.keys())\n",
    "    le = LabelEncoder()\n",
    "    for feature in le_features:\n",
    "        le.fit(encoding_order[feature])\n",
    "        df[feature] = le.transform(df[feature])\n",
    "    \"\"\"\n",
    "    return df\n",
    "\n",
    "def one_hot_encoding(df_old):\n",
    "    df = df_old.copy()\n",
    "    \n",
    "    # For NAME_FAMILY_STATUS column, group 'Unknown' categorical value to 'Married' which is the mode since there is only 2 rows\n",
    "    df['NAME_FAMILY_STATUS'] = df['NAME_FAMILY_STATUS'].replace({'Unknown': 'Married'})\n",
    "    \n",
    "    column_list = [\n",
    "        \"NAME_CONTRACT_TYPE\", \"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\", \n",
    "        \"NAME_HOUSING_TYPE\", \"NAME_FAMILY_STATUS\", \"OCCUPATION_TYPE\",\n",
    "        \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\"\n",
    "    ]\n",
    "    for col in column_list:\n",
    "        df = pd.get_dummies(df, columns=[col])\n",
    "    return df\n",
    "\n",
    "def create_new_features(df_old):\n",
    "    df = df_old.copy()\n",
    "    \n",
    "    # Create new feature column: TOTAL_DOCUMENTS that aggregates the 20 FLAG_DOCUMENT fields (2-21)\n",
    "    df['TOTAL_DOCUMENTS'] = df['FLAG_DOCUMENT_2'] + df['FLAG_DOCUMENT_3'] + df['FLAG_DOCUMENT_4'] + df['FLAG_DOCUMENT_5'] +\\\n",
    "                            df['FLAG_DOCUMENT_6'] + df['FLAG_DOCUMENT_7'] + df['FLAG_DOCUMENT_8'] + df['FLAG_DOCUMENT_9'] +\\\n",
    "                            df['FLAG_DOCUMENT_10'] + df['FLAG_DOCUMENT_11'] + df['FLAG_DOCUMENT_12'] + df['FLAG_DOCUMENT_13'] +\\\n",
    "                            df['FLAG_DOCUMENT_14'] + df['FLAG_DOCUMENT_15'] + df['FLAG_DOCUMENT_16'] + df['FLAG_DOCUMENT_17'] +\\\n",
    "                            df['FLAG_DOCUMENT_18'] + df['FLAG_DOCUMENT_19'] + df['FLAG_DOCUMENT_20'] + df['FLAG_DOCUMENT_21']\n",
    "    \n",
    "    #for i in range(2, 22):\n",
    "    #    df = df.drop('FLAG_DOCUMENT_' + str(i), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_useless_columns(df_old):\n",
    "    df = df_old.copy()\n",
    "    \n",
    "    remove_columns_list = [\n",
    "        \"APARTMENTS_AVG\", \"BASEMENTAREA_AVG\", \"YEARS_BEGINEXPLUATATION_AVG\", \"YEARS_BUILD_AVG\",\n",
    "        \"COMMONAREA_AVG\", \"ELEVATORS_AVG\", \"ENTRANCES_AVG\", \"FLOORSMAX_AVG\", \"FLOORSMIN_AVG\",\n",
    "        \"LANDAREA_AVG\", \"LIVINGAPARTMENTS_AVG\", \"LIVINGAREA_AVG\", \"NONLIVINGAPARTMENTS_AVG\", \"NONLIVINGAREA_AVG\",\n",
    "        \"APARTMENTS_MODE\", \"BASEMENTAREA_MODE\", \"YEARS_BEGINEXPLUATATION_MODE\", \"YEARS_BUILD_MODE\", \"COMMONAREA_MODE\",\n",
    "        \"ELEVATORS_MODE\", \"ENTRANCES_MODE\", \"FLOORSMAX_MODE\", \"FLOORSMIN_MODE\", \"LANDAREA_MODE\", \"LIVINGAPARTMENTS_MODE\",\n",
    "        \"LIVINGAREA_MODE\", \"NONLIVINGAPARTMENTS_MODE\", \"NONLIVINGAREA_MODE\", \"APARTMENTS_MEDI\", \"BASEMENTAREA_MEDI\",\n",
    "        \"YEARS_BEGINEXPLUATATION_MEDI\", \"YEARS_BUILD_MEDI\", \"COMMONAREA_MEDI\", \"ELEVATORS_MEDI\", \"ENTRANCES_MEDI\",\n",
    "        \"FLOORSMAX_MEDI\", \"FLOORSMIN_MEDI\", \"LANDAREA_MEDI\", \"LIVINGAPARTMENTS_MEDI\", \"LIVINGAREA_MEDI\",\n",
    "        \"NONLIVINGAPARTMENTS_MEDI\", \"NONLIVINGAREA_MEDI\", \"FONDKAPREMONT_MODE\", \"HOUSETYPE_MODE\", \"TOTALAREA_MODE\",\n",
    "        \"WALLSMATERIAL_MODE\", \"EMERGENCYSTATE_MODE\"\n",
    "    ]\n",
    "    for column_name in remove_columns_list:\n",
    "        df = df.drop(column_name, axis=1)\n",
    "    return df\n",
    "\n",
    "def agg_numeric(df, group_var, df_name):\n",
    "    \"\"\"Aggregates the numeric values in a dataframe. This can\n",
    "    be used to create features for each instance of the grouping variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the dataframe to calculate the statistics on\n",
    "        group_var (string): \n",
    "            the variable by which to group df\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated for \n",
    "            all numeric columns. Each instance of the grouping variable will have \n",
    "            the statistics (mean, min, max, sum; currently supported) calculated. \n",
    "            The columns are also renamed to keep track of features created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col, axis=1).copy()\n",
    "            \n",
    "    group_ids = df[group_var]\n",
    "    numeric_df = df.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(['mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = [group_var]\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        # Skip the grouping variable\n",
    "        if var != group_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1][:-1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "    return agg\n",
    "\n",
    "# For POS_CASH Balance Only\n",
    "\n",
    "def agg_numeric_with_count(df, group_var, df_name):\n",
    "            \n",
    "    group_ids = df[group_var]\n",
    "    numeric_df = df.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(['count', 'median', 'max']).reset_index()\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = [group_var]\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        # Skip the grouping variable\n",
    "        if var != group_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1][:-1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "    return agg\n",
    "\n",
    "def count_categorical(df, group_var, df_name):\n",
    "    \"\"\"Computes counts and normalized counts for each observation\n",
    "    of `group_var` of each unique category in every categorical variable\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    group_var : string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
    "        with one row for every unique value of the `group_var`.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(df.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[group_var] = df[group_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['count', 'count_norm']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    return categorical\n",
    "\n",
    "# New Features & Pre-processing for Previous Application data\n",
    "\n",
    "def prev_app_process(df_prev_app):\n",
    "    \n",
    "    # 1. Only consider the final application recrods (with flag of last application = Y)\n",
    "    \n",
    "    print('Before Apply Filter: ' + str(len(df_prev_app)))\n",
    "    \n",
    "    df_prev_app = df_prev_app[df_prev_app['FLAG_LAST_APPL_PER_CONTRACT']=='Y'].copy()\n",
    "    \n",
    "    print('After Apply Filter: ' + str(len(df_prev_app)))\n",
    "    \n",
    "    # 2. Drop the two flags as not important anymore (shd be always = Y )\n",
    "    # 3. Only Consumer Loan has columns interest rate data, 99% missing => Drop\n",
    "    df_prev_app = df_prev_app.drop(['FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY','RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED'],axis=1).copy()    \n",
    "    \n",
    "    # 4. Fill in 0 for missing down payment (i.e. no down payment / not required = down payment)\n",
    "    df_prev_app['RATE_DOWN_PAYMENT']=df_prev_app['RATE_DOWN_PAYMENT'].fillna(0).copy()\n",
    "    \n",
    "    # 5. Ratio of Credit amount received / Applied\n",
    "    #    To avoid error, fill missing to be 0\n",
    "    #    when AMT_APPLICATION = 0, default the ratio to be 0 (i.e. not fulfil application)\n",
    "    df_prev_app['AMT_CREDIT']=df_prev_app['AMT_CREDIT'].fillna(0).copy()\n",
    "    #df_prev_app['AMT_CREDIT_APPLICATION_RATIO'] = np.where(df_prev_app['AMT_APPLICATION']==0, df_prev_app['AMT_CREDIT'] / df_prev_app['AMT_APPLICATION'],0)\n",
    "    df_prev_app['AMT_CREDIT_APPLICATION_RATIO'] = np.where(df_prev_app['AMT_APPLICATION']==0, 0, df_prev_app['AMT_CREDIT'] / df_prev_app['AMT_APPLICATION'])\n",
    "    \n",
    "    # 6. Transform DAYS_DECISION from days to month\n",
    "    df_prev_app['APPLICATION_NO_OF_MONTH_AGO']=(-df_prev_app['DAYS_DECISION']/30).apply(np.ceil)\n",
    "    \n",
    "    # 7. Replace 365243 to be nan\n",
    "    for col in ('DAYS_FIRST_DRAWING','DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION'):\n",
    "\n",
    "        df_prev_app.replace({col:{365243 : np.nan,}}, inplace=True)\n",
    "    \n",
    "    # 8. Fill -1 to missing value in field: NFLAG_INSURED_ON_APPROVAL\n",
    "    df_prev_app['NFLAG_INSURED_ON_APPROVAL']=df_prev_app['NFLAG_INSURED_ON_APPROVAL'].fillna(-1).copy()\n",
    "    \n",
    "    return df_prev_app\n",
    "\n",
    "\n",
    "def prev_application_Summary(df_prev):\n",
    "    \n",
    "    processed = prev_app_process(df_prev)\n",
    "    \n",
    "    prev_app_count = count_categorical(processed.drop('SK_ID_PREV',axis=1), group_var = 'SK_ID_CURR', df_name = 'prev_app').reset_index()\n",
    "    \n",
    "    prev_app_agg = agg_numeric(processed.drop('SK_ID_PREV',axis=1), group_var = 'SK_ID_CURR', df_name = 'prev_app')\n",
    "    \n",
    "    # Get the count of previous applications within last month, 6 months, 1 year(only consider the last one for the contract)\n",
    "    for i in [1,6,12]:\n",
    "        \n",
    "        temp = processed[processed['APPLICATION_NO_OF_MONTH_AGO'] <= i]\n",
    "        \n",
    "        if i == 1:\n",
    "            \n",
    "            prev_app_count_record = temp.groupby('SK_ID_CURR').size().reset_index(name='prev_app_'+str(i)+'_Month_Count')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            temp_count = temp.groupby('SK_ID_CURR').size().reset_index(name='prev_app_'+str(i)+'_Month_Count')\n",
    "            \n",
    "            prev_app_count_record=prev_app_count_record.merge(temp_count,  \n",
    "                                                              on = 'SK_ID_CURR', \n",
    "                                                              how = 'outer')\n",
    "\n",
    "    # Merge the three tables tgt\n",
    "    \n",
    "    output = prev_app_count.merge(prev_app_agg, on='SK_ID_CURR', how='outer')\n",
    "    \n",
    "    output = output.merge(prev_app_count_record, on='SK_ID_CURR', how='outer').copy()\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "# installment feature creation\n",
    "\n",
    "def installment_process(df_install):\n",
    "    \n",
    "    # 0. NUM_INSTALMENT_VERSION 0,1,Others\n",
    "    \n",
    "    # 1. Fill NA with 0\n",
    "    # i.e. up till now still no payment\n",
    "    df_install['DAYS_ENTRY_PAYMENT']=df_install['DAYS_ENTRY_PAYMENT'].fillna(0).copy()\n",
    "    df_install['AMT_PAYMENT']=df_install['AMT_PAYMENT'].fillna(0).copy()\n",
    "    \n",
    "    # 2. Late Payment Days\n",
    "    # =max(DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT,0)\n",
    "    df_install['DAYS_LATE_PAYMENT']=np.where(df_install['DAYS_ENTRY_PAYMENT'] - df_install['DAYS_INSTALMENT'] > 0, df_install['DAYS_ENTRY_PAYMENT'] - df_install['DAYS_INSTALMENT'], 0)\n",
    "    \n",
    "    # 3. AMT_PAYMENT_INSTALMENT_RATIO\n",
    "    df_install['AMT_PAYMENT_INSTALMENT_RATIO'] = np.where(df_install['AMT_INSTALMENT']==0, 1, df_install['AMT_PAYMENT']/df_install['AMT_INSTALMENT'])\n",
    "    \n",
    "    # 4. Drop NUM_INSTALMENT_NUMBER\n",
    "    df_install = df_install.drop('NUM_INSTALMENT_NUMBER', axis=1).copy()\n",
    "    \n",
    "    # 5. Group NUM_INSTALMENT_VERSION into 3 groups: 0,1,Others\n",
    "    df_install.loc[df_install['NUM_INSTALMENT_VERSION'] >1,'NUM_INSTALMENT_VERSION']='Others'\n",
    "    \n",
    "    \n",
    "    return df_install\n",
    "\n",
    "\n",
    "# Installment Summary\n",
    "\n",
    "def install_summary(df_install):\n",
    "    \n",
    "    processed = installment_process(df_install)\n",
    "    \n",
    "    install_count = count_categorical(processed.drop('SK_ID_PREV',axis=1), group_var = 'SK_ID_CURR', df_name = 'install').reset_index()\n",
    "    \n",
    "    install_agg = agg_numeric(processed.drop('SK_ID_PREV',axis=1), group_var = 'SK_ID_CURR', df_name = 'install')\n",
    "\n",
    "    # Merge the 2 tables tgt\n",
    "    \n",
    "    output = install_count.merge(install_agg, on='SK_ID_CURR', how='outer')\n",
    "        \n",
    "    return output\n",
    "\n",
    "# Credit Card table preprocessing\n",
    "\n",
    "def credit_card_process(df_credit_card):\n",
    "    \n",
    "    # 0. Fill Na with 0 for Current Drawing releated fields & AMT_PAYMENT_CURRENT \n",
    "    for col in ['AMT_DRAWINGS_ATM_CURRENT', 'AMT_DRAWINGS_OTHER_CURRENT', 'AMT_DRAWINGS_POS_CURRENT', 'AMT_PAYMENT_CURRENT', 'CNT_DRAWINGS_ATM_CURRENT', 'CNT_DRAWINGS_OTHER_CURRENT','CNT_DRAWINGS_POS_CURRENT']:\n",
    "        df_credit_card[col]=df_credit_card[col].fillna(0).copy() \n",
    "    \n",
    "    # 1. Drop: AMT_RECEIVABLE_PRINCIPAL\n",
    "    #          AMT_RECIVABLE\n",
    "    #          AMT_TOTAL_RECEIVABLE\n",
    "    #          Can be well represented by AMT_BALANCE\n",
    "    df_credit_card = df_credit_card.drop(['AMT_RECEIVABLE_PRINCIPAL','AMT_RECIVABLE','AMT_TOTAL_RECEIVABLE'],axis=1).copy()\n",
    "    \n",
    "    \n",
    "    # 2. Combine AMT_PAYMENT_CURRENT & AMT_PAYMENT_TOTAL_CURRENT (Pick Max Value)\n",
    "    #    Then drop the two fields\n",
    "    df_credit_card['AMT_PAYMENT']=np.where(df_credit_card['AMT_PAYMENT_TOTAL_CURRENT'] > df_credit_card['AMT_PAYMENT_CURRENT'], df_credit_card['AMT_PAYMENT_CURRENT'], df_credit_card['AMT_PAYMENT_CURRENT'])\n",
    "    df_credit_card = df_credit_card.drop(['AMT_PAYMENT_TOTAL_CURRENT', 'AMT_PAYMENT_CURRENT'], axis=1).copy()\n",
    "    \n",
    "    \n",
    "    # 3. Average_Drawing_For_Goods = AMT_DRAWINGS_POS_CURRENT / CNT_DRAWINGS_POS_CURRENT\n",
    "    #    Average_Drawing = AMT_DRAWINGS_CURRENT / CNT_DRAWINGS_CURRENT\n",
    "    df_credit_card['Average_Drawing_For_Goods'] = np.where(df_credit_card['CNT_DRAWINGS_POS_CURRENT']==0, 0, df_credit_card['AMT_DRAWINGS_POS_CURRENT']/df_credit_card['CNT_DRAWINGS_POS_CURRENT'])\n",
    "    df_credit_card['Average_Drawing'] = np.where(df_credit_card['CNT_DRAWINGS_CURRENT']==0, 0, df_credit_card['AMT_DRAWINGS_CURRENT']/df_credit_card['CNT_DRAWINGS_CURRENT'])\n",
    "\n",
    "    \n",
    "    # 4. AMT_DRAWINGS_CURRENT ~ AMT_DRAWINGS_POS_CURRENT + AMT_DRAWINGS_ATM_CURRENT + AMT_DRAWINGS_OTHER_CURRENT\n",
    "    # Fill missing values of the three columns as 0\n",
    "    # New Feature: Special_Expense = difference between RHS & LHS\n",
    "    # Then Drop AMT_DRAWINGS_ATM_CURRENT, AMT_DRAWINGS_OTHER_CURRENT\n",
    "    df_credit_card['Special_Expense'] = (df_credit_card['AMT_DRAWINGS_CURRENT'] - df_credit_card['AMT_DRAWINGS_POS_CURRENT'] - df_credit_card['AMT_DRAWINGS_ATM_CURRENT'] - df_credit_card['AMT_DRAWINGS_OTHER_CURRENT']).abs()\n",
    "    df_credit_card = df_credit_card.drop(['AMT_DRAWINGS_ATM_CURRENT', 'AMT_DRAWINGS_OTHER_CURRENT'], axis=1).copy()\n",
    "    \n",
    "    \n",
    "    # 4. Calculate the ratio\n",
    "    #    Drawing_For_Goods_RATIO = CNT_DRAWINGS_POS_CURRENT / CNT_DRAWINGS_CURRENT\n",
    "    # Drop:\n",
    "    # CNT_DRAWINGS_ATM_CURRENT\n",
    "    # CNT_DRAWINGS_CURRENT\n",
    "    # CNT_DRAWINGS_OTHER_CURRENT\n",
    "    # CNT_DRAWINGS_POS_CURRENT\n",
    "    df_credit_card['Drawing_For_Goods_RATIO'] = np.where(df_credit_card['CNT_DRAWINGS_CURRENT']==0, 0, df_credit_card['CNT_DRAWINGS_POS_CURRENT'] / df_credit_card['CNT_DRAWINGS_CURRENT'])\n",
    "    df_credit_card = df_credit_card.drop(['CNT_DRAWINGS_ATM_CURRENT', 'CNT_DRAWINGS_CURRENT', 'CNT_DRAWINGS_OTHER_CURRENT', 'CNT_DRAWINGS_POS_CURRENT'], axis=1).copy()    \n",
    "    \n",
    "    \n",
    "    # 5. Drop column: MONTHS_BALANCE\n",
    "    df_credit_card=df_credit_card.drop('MONTHS_BALANCE', axis=1).copy()\n",
    "    \n",
    "    return df_credit_card\n",
    "\n",
    "\n",
    "# Credit Card Balance Summary\n",
    "\n",
    "def credit_bal_summary(df_credit_card):\n",
    "    \n",
    "    # Pre-process / Feature Creation\n",
    "    processed = credit_card_process(df_credit_card)\n",
    "    \n",
    "    # Categorical Variables Aggregration\n",
    "    credit_count = count_categorical(processed.drop('SK_ID_PREV',axis=1), group_var = 'SK_ID_CURR', df_name = 'credit_bal').reset_index()\n",
    "    \n",
    "    # Numeric Variables Aggregration\n",
    "    credit_agg = agg_numeric(processed.drop('SK_ID_PREV',axis=1), group_var = 'SK_ID_CURR', df_name = 'credit_bal')\n",
    "\n",
    "    # Merge the 2 tables tgt\n",
    "    output = credit_count.merge(credit_agg, on='SK_ID_CURR', how='outer')\n",
    "        \n",
    "    return output\n",
    "\n",
    "# POS Cash Summary\n",
    "\n",
    "def POS_Cash_Summary(df_pos_cash):\n",
    "    \n",
    "    # Mainly just fill in missing value\n",
    "    df_pos_cash=df_pos_cash=df_pos_cash.fillna(0).drop('SK_ID_PREV',axis=1).copy()\n",
    "    \n",
    "    # 1. Extract only for those contracts as of now\n",
    "    Last_Bal = df_pos_cash[df_pos_cash['MONTHS_BALANCE']==-1].drop('MONTHS_BALANCE', axis=1)\n",
    "    \n",
    "    # Categorical Variables Aggregration\n",
    "    Last_Bal_count = count_categorical(Last_Bal, group_var = 'SK_ID_CURR', df_name = 'pos_cash_last_bal').reset_index()\n",
    "    \n",
    "    # Numeric Variables Aggregration\n",
    "    Last_Bal_agg = agg_numeric(Last_Bal, group_var = 'SK_ID_CURR', df_name = 'pos_cash_last_bal')\n",
    "\n",
    "    # Merge the 2 tables tgt\n",
    "    Last_Bal_output = Last_Bal_count.merge(Last_Bal_agg, on='SK_ID_CURR', how='outer')\n",
    "    \n",
    "    # 2. Overall POS Cash Balance Past Due Day Issue\n",
    "    # - Count how many months bal with DPD\n",
    "    # - Median\n",
    "    # - Max\n",
    "    \n",
    "    With_DPD = df_pos_cash[df_pos_cash['SK_DPD'] > 0][['SK_ID_CURR', 'SK_DPD', 'SK_DPD_DEF']]\n",
    "    \n",
    "    With_DPD_agg = agg_numeric_with_count(With_DPD, group_var = 'SK_ID_CURR', df_name = 'pos_cash_DPD')\n",
    "    \n",
    "    output =  Last_Bal_output.merge(With_DPD_agg, on='SK_ID_CURR', how='outer')\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def get_bureau_all_df(bureau, bureau_balance):\n",
    "\n",
    "    bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\n",
    "\n",
    "    bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\n",
    "    \n",
    "    bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
    "\n",
    "    bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n",
    "\n",
    "    # Dataframe grouped by the loan\n",
    "    bureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n",
    "    \n",
    "    # Merge to include the SK_ID_CURR\n",
    "    bureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n",
    "    \n",
    "    # Aggregate the stats for each client\n",
    "    bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\n",
    "\n",
    "    train_bureau = bureau_counts.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "    # Merge with the monthly information grouped by client\n",
    "    train_bureau = train_bureau.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "    return   train_bureau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all CSV Files\n",
    "train = pd.read_csv(\"application_train.csv\")\n",
    "test = pd.read_csv(\"application_test.csv\")\n",
    "application = pd.concat([train, test], ignore_index=True, sort=False)\n",
    "\n",
    "# Preprocess application train and test CSV\n",
    "application = fill_missing_values(application)\n",
    "application = label_encoding(application)\n",
    "application = one_hot_encoding(application)\n",
    "application = create_new_features(application)\n",
    "application = remove_useless_columns(application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Apply Filter: 1670214\n",
      "After Apply Filter: 1661739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khlau/Documents/msbd5012/lib/python3.6/site-packages/ipykernel_launcher.py:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess previous application CSV\n",
    "prev_app = pd.read_csv('previous_application.csv')\n",
    "prev_app_for_merge = prev_application_Summary(prev_app)\n",
    "del prev_app\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khlau/Documents/msbd5012/lib/python3.6/site-packages/ipykernel_launcher.py:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess installment payments CSV\n",
    "installment = pd.read_csv('installments_payments.csv')\n",
    "install_for_merge = install_summary(installment)\n",
    "del installment\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khlau/Documents/msbd5012/lib/python3.6/site-packages/ipykernel_launcher.py:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess credit card balance CSV\n",
    "credit_bal = pd.read_csv('credit_card_balance.csv')\n",
    "credit_bal_for_merge = credit_bal_summary(credit_bal)\n",
    "del credit_bal\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khlau/Documents/msbd5012/lib/python3.6/site-packages/ipykernel_launcher.py:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cash = pd.read_csv('POS_CASH_balance.csv')\n",
    "# Preprocess POS Cash Balance CSV\n",
    "POS_cash_for_merge = POS_Cash_Summary(pos_cash)\n",
    "del pos_cash\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khlau/Documents/msbd5012/lib/python3.6/site-packages/ipykernel_launcher.py:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess bureau and bureau balance CSV\n",
    "bureau = pd.read_csv('bureau.csv')\n",
    "bureau_balance = pd.read_csv('bureau_balance.csv')\n",
    "bureau_for_merge = get_bureau_all_df(bureau, bureau_balance)\n",
    "del bureau,bureau_balance\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all tables into one main table\n",
    "main_table = application.merge(prev_app_for_merge, on = 'SK_ID_CURR', how = 'left')\n",
    "del prev_app_for_merge\n",
    "gc.collect()\n",
    "\n",
    "main_table = main_table.merge(install_for_merge, on = 'SK_ID_CURR', how = 'left')\n",
    "del install_for_merge\n",
    "gc.collect()\n",
    "\n",
    "main_table = main_table.merge(credit_bal_for_merge, on = 'SK_ID_CURR', how = 'left')\n",
    "del credit_bal_for_merge\n",
    "gc.collect()\n",
    "\n",
    "main_table = main_table.merge(POS_cash_for_merge, on = 'SK_ID_CURR', how = 'left')\n",
    "del POS_cash_for_merge\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "main_table = main_table.merge(bureau_for_merge, on = 'SK_ID_CURR', how = 'left')\n",
    "del bureau_for_merge\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column TARGET from grand table not processed\n"
     ]
    }
   ],
   "source": [
    "main_table = fill_all_missing_values(main_table)\n",
    "\n",
    "main_table_train = main_table[main_table['TARGET'].notnull()]\n",
    "main_table_test = main_table[main_table['TARGET'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table_train.to_csv(\"main_table2.csv\")\n",
    "main_table_test.to_csv(\"test_table2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356255, 180)\n",
      "(338857, 358)\n",
      "(339587, 31)\n",
      "(103558, 67)\n",
      "(127035, 37)\n",
      "(305811, 175)\n",
      "(356255, 843)\n"
     ]
    }
   ],
   "source": [
    "print(application.shape)\n",
    "print(prev_app_for_merge.shape)\n",
    "print(install_for_merge.shape)\n",
    "print(credit_bal_for_merge.shape)\n",
    "print(POS_cash_for_merge.shape)\n",
    "print(bureau_for_merge.shape)\n",
    "print(main_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    282686\n",
      "1     24825\n",
      "Name: TARGET, dtype: int64\n",
      "0    0.919271\n",
      "1    0.080729\n",
      "Name: TARGET, dtype: float64\n",
      "Missing Value: 0/307511 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# TARGET\n",
    "# Target variable \n",
    "# 1 - client with payment difficulties: he/she had late payment more than X days\n",
    "# on at least one of the first Y installments of the loan in our sample\n",
    "# 0 - all other cases\n",
    "# Only 8% has 1 as the value so need to use oversampling\n",
    "col_name=\"TARGET\"\n",
    "print(train[col_name].value_counts())\n",
    "print(train[col_name].value_counts() / len(train[col_name]))\n",
    "\n",
    "missing_count = train[col_name].isnull().sum()\n",
    "total_count = len(train[col_name])\n",
    "print(\"Missing Value: %d/%d (%f)\" % (missing_count, total_count, missing_count / total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unaccompanied      248526\n",
      "Family              40149\n",
      "Spouse, partner     11370\n",
      "Children             3267\n",
      "Other_B              1770\n",
      "Other_A               866\n",
      "Group of people       271\n",
      "Name: NAME_TYPE_SUITE, dtype: int64\n",
      "Missing Value: 1292/307511 (0.004201)\n"
     ]
    }
   ],
   "source": [
    "# NAME_TYPE_SUITE\n",
    "# Who was accompanying client when he was applying for the loan\n",
    "# - don't think it is very related\n",
    "# - should be categorical so use one-hot\n",
    "# - consider dropping this column since the information does not seems to be very useful\n",
    "col_name = \"NAME_TYPE_SUITE\"\n",
    "print(train[col_name].value_counts())\n",
    "missing_count = train[col_name].isnull().sum()\n",
    "total_count = len(train[col_name])\n",
    "print(\"Missing Value: %d/%d (%f)\" % (missing_count, total_count, missing_count / total_count))\n",
    "\n",
    "# Convert each categorical value into a new column using one-hot method\n",
    "#train = pd.get_dummies(train, columns=[\"NAME_TYPE_SUITE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working                 158774\n",
      "Commercial associate     71617\n",
      "Pensioner                55362\n",
      "State servant            21703\n",
      "Unemployed                  22\n",
      "Student                     18\n",
      "Businessman                 10\n",
      "Maternity leave              5\n",
      "Name: NAME_INCOME_TYPE, dtype: int64\n",
      "Missing Value: 0/307511 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# NAME_INCOME_TYPE\n",
    "# Clients income type (businessman, working, maternity leave,?)\n",
    "col_name = \"NAME_INCOME_TYPE\"\n",
    "print(train[col_name].value_counts())\n",
    "missing_count = train[col_name].isnull().sum()\n",
    "total_count = len(train[col_name])\n",
    "print(\"Missing Value: %d/%d (%f)\" % (missing_count, total_count, missing_count / total_count))\n",
    "\n",
    "# Convert each categorical value into a new column using one-hot method\n",
    "#train = pd.get_dummies(train, columns=[\"NAME_INCOME_TYPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary / secondary special    218391\n",
      "Higher education                  74863\n",
      "Incomplete higher                 10277\n",
      "Lower secondary                    3816\n",
      "Academic degree                     164\n",
      "Name: NAME_EDUCATION_TYPE, dtype: int64\n",
      "Missing value: 0\n"
     ]
    }
   ],
   "source": [
    "# NAME_EDUCATION_TYPE\n",
    "# Level of highest education the client achieved\n",
    "# - should be ordinal\n",
    "# - maybe just categorical?\n",
    "# - no missing value\n",
    "# Academic degree: 1\n",
    "# Lower secondary: 2\n",
    "# Secondary / secondary special: 3\n",
    "# Incomplete higher: 4\n",
    "# Higher education: 5\n",
    "col_name = \"NAME_EDUCATION_TYPE\"\n",
    "print(train[col_name].value_counts())\n",
    "print(\"Missing value: \" + str(train[col_name].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Married                 196432\n",
      "Single / not married     45444\n",
      "Civil marriage           29775\n",
      "Separated                19770\n",
      "Widow                    16088\n",
      "Unknown                      2\n",
      "Name: NAME_FAMILY_STATUS, dtype: int64\n",
      "Missing value: 0\n"
     ]
    }
   ],
   "source": [
    "# NAME_FAMILY_STATUS\n",
    "# Family status of the client\n",
    "# Categorical and no missing value, so one-hot\n",
    "col_name = \"NAME_FAMILY_STATUS\"\n",
    "print(train[col_name].value_counts())\n",
    "print(\"Missing value: \" + str(train[col_name].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House / apartment      272868\n",
      "With parents            14840\n",
      "Municipal apartment     11183\n",
      "Rented apartment         4881\n",
      "Office apartment         2617\n",
      "Co-op apartment          1122\n",
      "Name: NAME_HOUSING_TYPE, dtype: int64\n",
      "Missing value: 0\n"
     ]
    }
   ],
   "source": [
    "# NAME_HOUSING_TYPE\n",
    "# \"What is the housing situation of the client (renting, living with parents, ...)\"\n",
    "# - categorical with no missing value so one-hot\n",
    "col_name = \"NAME_HOUSING_TYPE\"\n",
    "print(train[col_name].value_counts())\n",
    "print(\"Missing value: \" + str(train[col_name].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REGION_POPULATION_RELATIVE,\n",
    "# Normalized population of region where client lives \n",
    "# (higher number means the client lives in more populated region),normalized\n",
    "# - Richer people usually living in less dense populated region, not necessary for rural \n",
    "# - negative correlation with target\n",
    "print(\"Correlation with target: %f\" % (train[col_name].corr(train['TARGET'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DAYS_BIRTH\n",
    "# Client's age in days at the time of application,time only relative to the application\n",
    "# This is an important feature since older people tends to be able to pay loan\n",
    "# Number is backward starting from application date so all numbers are negative\n",
    "# From age 20 (-7489) to age 69 (-25229)\n",
    "# The correlation is positive, but the value of this feature is actually negative, \n",
    "# meaning that as the client gets older, they are less likely to default on their loan\n",
    "# (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative.\n",
    "col_name = \"DAYS_BIRTH\"\n",
    "print(train[col_name].describe())\n",
    "print(\"Missing value: \" + str(train[col_name].isnull().sum()))\n",
    "\n",
    "# Find the correlation of the positive days since birth and target\n",
    "# As the client gets older, there is a negative linear relationship with the target \n",
    "# meaning that as clients get older, they tend to repay their loans on time more often.\n",
    "train[col_name] = abs(train[col_name])\n",
    "print(\"Correlation with target: %f\" % (train[col_name].corr(train['TARGET'])))\n",
    "\n",
    "# A clear trend: \n",
    "# younger applicants are more likely to not repay the loan! \n",
    "# The rate of failure to repay is above 10% for the youngest three age groups \n",
    "# and beolow 5% for the oldest age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    307511.000000\n",
      "mean      63815.045904\n",
      "std      141275.766519\n",
      "min      -17912.000000\n",
      "25%       -2760.000000\n",
      "50%       -1213.000000\n",
      "75%        -289.000000\n",
      "max      365243.000000\n",
      "Name: DAYS_EMPLOYED, dtype: float64\n",
      "Missing value: 55374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Days Employment')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DAYS_EMPLOYED\n",
    "# How many days before the application the person started current employment\n",
    "# time only relative to the application\n",
    "# - Value 365243 does not make sense and contain a count of 55374\n",
    "# - should be consider missing value\n",
    "# - use 0 to replace it perhaps?\n",
    "\n",
    "col_name = \"DAYS_EMPLOYED\"\n",
    "print(train[col_name].describe())\n",
    "\n",
    "# Create an anomalous flag column\n",
    "train['DAYS_EMPLOYED_ANOM'] = train[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "print(\"Missing value: \" + str(train[col_name].isnull().sum()))\n",
    "\n",
    "train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\n",
    "plt.xlabel('Days Employment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DAYS_REGISTRATION\n",
    "# How many days before the application did client change his registration\n",
    "# time only relative to the application\n",
    "# - not sure what registration mean but it is already numerical\n",
    "print(\"Correlation with target: %f\" % (train[\"DAYS_REGISTRATION\"].corr(train['TARGET'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DAYS_ID_PUBLISH\n",
    "# How many days before the application did client change the identity document\n",
    "# with which he applied for the loan, time only relative to the application\n",
    "# - Seems to be quite important\n",
    "# - Not sure what that means, how come someone can change his/her identity doc?\n",
    "print(\"Correlation with target: %f\" % (train[\"DAYS_ID_PUBLISH\"].corr(train['TARGET'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    104582.000000\n",
      "mean         12.061091\n",
      "std          11.944812\n",
      "min           0.000000\n",
      "25%           5.000000\n",
      "50%           9.000000\n",
      "75%          15.000000\n",
      "max          91.000000\n",
      "Name: OWN_CAR_AGE, dtype: float64\n",
      "Missing Value: 202929/307511 (0.659908)\n"
     ]
    }
   ],
   "source": [
    "# OWN_CAR_AGE\n",
    "# Age of client's car\n",
    "# - need to decide whether to use imputation to fill in missing value or remove the entire column\n",
    "# - another alternative is to use algorithm that can handle missing value e.g. XGBoost\n",
    "# fill missing value with max\n",
    "col_name = \"OWN_CAR_AGE\"\n",
    "print(train[col_name].describe())\n",
    "missing_count = train[col_name].isnull().sum()\n",
    "total_count = len(train[col_name])\n",
    "print(\"Missing Value: %d/%d (%f)\" % (missing_count, total_count, missing_count / total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0     7424\n",
       "6.0     6382\n",
       "3.0     6370\n",
       "8.0     5887\n",
       "2.0     5852\n",
       "4.0     5557\n",
       "1.0     5280\n",
       "9.0     5020\n",
       "10.0    4806\n",
       "14.0    4594\n",
       "13.0    4566\n",
       "12.0    4257\n",
       "11.0    4161\n",
       "5.0     3595\n",
       "15.0    3580\n",
       "16.0    3355\n",
       "17.0    2899\n",
       "64.0    2443\n",
       "18.0    2418\n",
       "0.0     2134\n",
       "19.0    1864\n",
       "20.0    1527\n",
       "21.0    1462\n",
       "22.0    1250\n",
       "24.0    1150\n",
       "23.0    1067\n",
       "65.0     891\n",
       "25.0     865\n",
       "26.0     580\n",
       "28.0     542\n",
       "        ... \n",
       "30.0     326\n",
       "31.0     267\n",
       "32.0     208\n",
       "34.0     183\n",
       "35.0     157\n",
       "33.0     132\n",
       "36.0     124\n",
       "38.0      97\n",
       "40.0      85\n",
       "39.0      78\n",
       "37.0      75\n",
       "41.0      58\n",
       "42.0      42\n",
       "44.0      21\n",
       "43.0      19\n",
       "54.0      12\n",
       "45.0      11\n",
       "49.0       6\n",
       "46.0       5\n",
       "55.0       4\n",
       "51.0       3\n",
       "63.0       2\n",
       "91.0       2\n",
       "69.0       1\n",
       "48.0       1\n",
       "52.0       1\n",
       "56.0       1\n",
       "47.0       1\n",
       "50.0       1\n",
       "57.0       1\n",
       "Name: OWN_CAR_AGE, Length: 62, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"OWN_CAR_AGE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13605401\n"
     ]
    }
   ],
   "source": [
    "payment = pd.read_csv(\"installments_payments.csv\")\n",
    "payment.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13605401\n"
     ]
    }
   ],
   "source": [
    "print(len(payment[\"AMT_PAYMENT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMT_PAYMENT_CURRENT           767988\n",
      "AMT_DRAWINGS_OTHER_CURRENT    749816\n",
      "CNT_DRAWINGS_POS_CURRENT      749816\n",
      "CNT_DRAWINGS_OTHER_CURRENT    749816\n",
      "CNT_DRAWINGS_ATM_CURRENT      749816\n",
      "AMT_DRAWINGS_ATM_CURRENT      749816\n",
      "AMT_DRAWINGS_POS_CURRENT      749816\n",
      "CNT_INSTALMENT_MATURE_CUM     305236\n",
      "AMT_INST_MIN_REGULARITY       305236\n",
      "SK_DPD_DEF                         0\n",
      "SK_ID_CURR                         0\n",
      "MONTHS_BALANCE                     0\n",
      "AMT_BALANCE                        0\n",
      "AMT_CREDIT_LIMIT_ACTUAL            0\n",
      "AMT_DRAWINGS_CURRENT               0\n",
      "AMT_PAYMENT_TOTAL_CURRENT          0\n",
      "SK_DPD                             0\n",
      "AMT_RECEIVABLE_PRINCIPAL           0\n",
      "AMT_RECIVABLE                      0\n",
      "AMT_TOTAL_RECEIVABLE               0\n",
      "CNT_DRAWINGS_CURRENT               0\n",
      "NAME_CONTRACT_STATUS               0\n",
      "SK_ID_PREV                         0\n",
      "dtype: int64\n",
      "3840312\n"
     ]
    }
   ],
   "source": [
    "credit_card = pd.read_csv(\"credit_card_balance.csv\")\n",
    "print(credit_card.isnull().sum().sort_values(ascending=False))\n",
    "print(len(credit_card[\"SK_ID_CURR\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msbd5012",
   "language": "python",
   "name": "msbd5012"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
